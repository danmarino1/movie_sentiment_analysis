{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few records of the training data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase   \n",
       "0         1           1  A series of escapades demonstrating the adage ...  \\\n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.tsv.zip', sep='\\t')\n",
    "test_df = pd.read_csv('test.tsv.zip', sep='\\t')\n",
    "\n",
    "print(\"First few records of the training data:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few records of the test data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"First few records of the test data:\")\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in training data: PhraseId      0\n",
      "SentenceId    0\n",
      "Phrase        0\n",
      "Sentiment     0\n",
      "dtype: int64\n",
      "Null values in test data: PhraseId      0\n",
      "SentenceId    0\n",
      "Phrase        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Null values in training data:\", train_df.isnull().sum())\n",
    "print(\"Null values in test data:\", test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types in training data: PhraseId       int64\n",
      "SentenceId     int64\n",
      "Phrase        object\n",
      "Sentiment      int64\n",
      "dtype: object\n",
      "Data types in test data: PhraseId       int64\n",
      "SentenceId     int64\n",
      "Phrase        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types in training data:\", train_df.dtypes)\n",
    "print(\"Data types in test data:\", test_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', min_df=2)\n",
    "X_train_counts = vectorizer.fit_transform(train_df['Phrase'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_counts, train_df['Sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Validation Set: 0.6426694860950917\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_val_pred = log_reg.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Model Accuracy on Validation Set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmarino/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "12 fits failed out of a total of 24.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/danmarino/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/danmarino/Library/Python/3.9/lib/python/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/danmarino/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/danmarino/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/danmarino/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.53554723        nan 0.59535595        nan 0.63341023\n",
      "        nan 0.62620947]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Grid Search: {'C': 1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42), param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters from Grid Search: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Model Accuracy on Validation Set: 0.6426694860950917\n"
     ]
    }
   ],
   "source": [
    "log_reg_optimized = LogisticRegression(C=best_params['C'], penalty=best_params['penalty'], max_iter=1000, random_state=42)\n",
    "log_reg_optimized.fit(X_train, y_train)\n",
    "y_val_pred_optimized = log_reg_optimized.predict(X_val)\n",
    "accuracy_optimized = accuracy_score(y_val, y_val_pred_optimized)\n",
    "print(f\"Optimized Model Accuracy on Validation Set: {accuracy_optimized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get the same results from the optimized model as we do for our original model. Other methods could be inplemented including using BERT for increased nuance detection\n"
     ]
    }
   ],
   "source": [
    "if accuracy_optimized == accuracy:\n",
    "    print(\"We get the same results from the optimized model as we do for our original model. Other methods could be inplemented including using BERT for increased nuance detection\")\n",
    "elif accuracy_optimized > accuracy:\n",
    "    print(\"We get better results from the optimized model than we do for our original model. Other methods could be inplemented including using BERT for increased nuance detection\")\n",
    "else:\n",
    "    print(\"We get worse results from the optimized model than we do for our original model. Other methods could be inplemented including using BERT for increased nuance detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/danmarino/Documents/GitHub/movie_sentiment_analysis/movie_sentiment_analysis.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danmarino/Documents/GitHub/movie_sentiment_analysis/movie_sentiment_analysis.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Make a submission file\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danmarino/Documents/GitHub/movie_sentiment_analysis/movie_sentiment_analysis.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X_test_counts \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mtransform(test_df[\u001b[39m'\u001b[39;49m\u001b[39mPhrase\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danmarino/Documents/GitHub/movie_sentiment_analysis/movie_sentiment_analysis.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_test_pred \u001b[39m=\u001b[39m log_reg_optimized\u001b[39m.\u001b[39mpredict(X_test_counts)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danmarino/Documents/GitHub/movie_sentiment_analysis/movie_sentiment_analysis.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m submission_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mPhraseId\u001b[39m\u001b[39m'\u001b[39m: test_df[\u001b[39m'\u001b[39m\u001b[39mPhraseId\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mSentiment\u001b[39m\u001b[39m'\u001b[39m: y_test_pred})\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/feature_extraction/text.py:1428\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1427\u001b[0m \u001b[39m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1428\u001b[0m _, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, fixed_vocab\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1429\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1430\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/feature_extraction/text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1269\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1270\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1271\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/feature_extraction/text.py:105\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m decoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     doc \u001b[39m=\u001b[39m decoder(doc)\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     doc \u001b[39m=\u001b[39m analyzer(doc)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/feature_extraction/text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    235\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_error)\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m doc \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mnan:\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# Make a submission file\n",
    "X_test_counts = vectorizer.transform(test_df['Phrase'])\n",
    "y_test_pred = log_reg_optimized.predict(X_test_counts)\n",
    "submission_df = pd.DataFrame({'PhraseId': test_df['PhraseId'], 'Sentiment': y_test_pred})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
