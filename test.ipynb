{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few records of the training data:\n",
      "   PhraseId  SentenceId                                             Phrase   \n",
      "0         1           1  A series of escapades demonstrating the adage ...  \\\n",
      "1         2           1  A series of escapades demonstrating the adage ...   \n",
      "2         3           1                                           A series   \n",
      "3         4           1                                                  A   \n",
      "4         5           1                                             series   \n",
      "\n",
      "   Sentiment  \n",
      "0          1  \n",
      "1          2  \n",
      "2          2  \n",
      "3          2  \n",
      "4          2  \n",
      "First few records of the test data:\n",
      "   PhraseId  SentenceId                                             Phrase\n",
      "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
      "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
      "2    156063        8545                                                 An\n",
      "3    156064        8545  intermittently pleasing but mostly routine effort\n",
      "4    156065        8545         intermittently pleasing but mostly routine\n",
      "Null values in training data: PhraseId      0\n",
      "SentenceId    0\n",
      "Phrase        0\n",
      "Sentiment     0\n",
      "dtype: int64\n",
      "Null values in test data: PhraseId      0\n",
      "SentenceId    0\n",
      "Phrase        1\n",
      "dtype: int64\n",
      "Data types in training data: PhraseId       int64\n",
      "SentenceId     int64\n",
      "Phrase        object\n",
      "Sentiment      int64\n",
      "dtype: object\n",
      "Data types in test data: PhraseId       int64\n",
      "SentenceId     int64\n",
      "Phrase        object\n",
      "dtype: object\n",
      "Model Accuracy on Validation Set: 0.6426694860950917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmarino/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "12 fits failed out of a total of 24.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/danmarino/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/danmarino/Library/Python/3.9/lib/python/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/danmarino/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/danmarino/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/danmarino/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.53554723        nan 0.59535595        nan 0.63341023\n",
      "        nan 0.62620947]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Grid Search: {'C': 1, 'penalty': 'l2'}\n",
      "Optimized Model Accuracy on Validation Set: 0.6426694860950917\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis on Movie Reviews\n",
    "\n",
    "## Import Required Libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## Step 1: Load and Explore the Data\n",
    "\n",
    "# Replace 'train_path' and 'test_path' with your actual file paths for the training and test datasets\n",
    "train_df = pd.read_csv('train.tsv.zip', sep='\\t')\n",
    "test_df = pd.read_csv('test.tsv.zip', sep='\\t')\n",
    "\n",
    "# Show the first few records of the training data\n",
    "print(\"First few records of the training data:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Show the first few records of the test data\n",
    "print(\"First few records of the test data:\")\n",
    "print(test_df.head())\n",
    "\n",
    "## Step 2: Handle Null Values\n",
    "\n",
    "# Check for null values in the training and test data\n",
    "print(\"Null values in training data:\", train_df.isnull().sum())\n",
    "print(\"Null values in test data:\", test_df.isnull().sum())\n",
    "\n",
    "# Since there are no null values, we don't need to handle them in this case.\n",
    "\n",
    "## Step 3: Inspect Data Types\n",
    "\n",
    "# Check the data types of the columns in training and test data\n",
    "print(\"Data types in training data:\", train_df.dtypes)\n",
    "print(\"Data types in test data:\", test_df.dtypes)\n",
    "\n",
    "## Step 4: Text Preprocessing\n",
    "\n",
    "# For simplicity, we'll move forward without additional text preprocessing steps like stemming or lemmatization.\n",
    "\n",
    "## Step 5: Feature Extraction\n",
    "\n",
    "# Initialize CountVectorizer to convert text into bag-of-words representation\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=2)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_counts = vectorizer.fit_transform(train_df['Phrase'])\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_counts, train_df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "## Step 6: Build the Logistic Regression Model\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = log_reg.predict(X_val)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Model Accuracy on Validation Set: {accuracy}\")\n",
    "\n",
    "## Step 7: Hyperparameter Tuning\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42), param_grid, cv=3)\n",
    "\n",
    "# Perform Grid Search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from Grid Search\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters from Grid Search: {best_params}\")\n",
    "\n",
    "## Step 8: Evaluate the Optimized Model\n",
    "\n",
    "# Initialize the Logistic Regression model with the best hyperparameters\n",
    "log_reg_optimized = LogisticRegression(C=best_params['C'], penalty=best_params['penalty'], max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the optimized model on the training data\n",
    "log_reg_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred_optimized = log_reg_optimized.predict(X_val)\n",
    "\n",
    "# Evaluate the optimized model's performance\n",
    "accuracy_optimized = accuracy_score(y_val, y_val_pred_optimized)\n",
    "print(f\"Optimized Model Accuracy on Validation Set: {accuracy_optimized}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
